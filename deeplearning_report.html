<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>deeplearning_report – Pokemon Go Battle Assistant</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-dfb324f25d9b1687192fa8be62ac8f9c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./deeplearning_report.html">Deep Learning Report</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Pokemon Go Battle Assistant</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Pokemon GO Battle Assistant</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./project_charta.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project Charta</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data_report.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Report</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./modelling_report.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Modelling Report</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./deeplearning_report.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Deep Learning Report</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Evaluation</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#deep-learning-report" id="toc-deep-learning-report" class="nav-link active" data-scroll-target="#deep-learning-report">Deep Learning Report</a>
  <ul class="collapse">
  <li><a href="#initial-situation" id="toc-initial-situation" class="nav-link" data-scroll-target="#initial-situation">Initial Situation</a></li>
  <li><a href="#model-descriptions" id="toc-model-descriptions" class="nav-link" data-scroll-target="#model-descriptions">Model Descriptions</a>
  <ul class="collapse">
  <li><a href="#cfar-model" id="toc-cfar-model" class="nav-link" data-scroll-target="#cfar-model">CFAR Model</a></li>
  <li><a href="#cnn-model" id="toc-cnn-model" class="nav-link" data-scroll-target="#cnn-model">CNN Model</a></li>
  <li><a href="#pre-trained-cnn-model-efficientnetb0-transfer-learning" id="toc-pre-trained-cnn-model-efficientnetb0-transfer-learning" class="nav-link" data-scroll-target="#pre-trained-cnn-model-efficientnetb0-transfer-learning">Pre-trained CNN Model (EfficientNetB0, Transfer Learning)</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#cfar-model-1" id="toc-cfar-model-1" class="nav-link" data-scroll-target="#cfar-model-1">CFAR Model</a></li>
  <li><a href="#cnn-model-1" id="toc-cnn-model-1" class="nav-link" data-scroll-target="#cnn-model-1">CNN Model</a></li>
  <li><a href="#pre-trained-cnn-model" id="toc-pre-trained-cnn-model" class="nav-link" data-scroll-target="#pre-trained-cnn-model">Pre-trained CNN Model</a></li>
  </ul></li>
  <li><a href="#model-interpretation" id="toc-model-interpretation" class="nav-link" data-scroll-target="#model-interpretation">Model Interpretation</a></li>
  <li><a href="#conclusions-and-next-steps" id="toc-conclusions-and-next-steps" class="nav-link" data-scroll-target="#conclusions-and-next-steps">Conclusions and Next Steps</a>
  <ul class="collapse">
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a></li>
  <li><a href="#future-work" id="toc-future-work" class="nav-link" data-scroll-target="#future-work">Future Work</a></li>
  <li><a href="#deployment" id="toc-deployment" class="nav-link" data-scroll-target="#deployment">Deployment</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="deeplearning_report.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="deep-learning-report" class="level1">
<h1>Deep Learning Report</h1>
<p>This report summarizes the details of the deep learning activities for Pokémon image classification.</p>
<section id="initial-situation" class="level2">
<h2 class="anchored" data-anchor-id="initial-situation">Initial Situation</h2>
<p>Our aim for the deep learning component is to accurately identify Pokémon from images, which complements the battle prediction system. This visual recognition capability allows users to upload images of Pokémon they encounter and get immediate battle strategy recommendations.</p>
<p>The dataset used consists of Pokémon images collected from various sources and organized into a structured format in the <code>data_acquisition/image_dataset/final_pokemon_dataset</code> directory, with separate train and test folders.</p>
<p>Our independent variables are the pixel values of the Pokémon images, and our dependent variable is the Pokémon species label. We created three different deep learning models: a CFAR (Convolutional Filter and Response) model, a custom CNN (Convolutional Neural Network), and a pre-trained transfer learning model using EfficientNetB0.</p>
<ul>
<li><strong>Aim of the modeling</strong>: Create an accurate image classifier that can identify Pokémon species from images</li>
<li><strong>Dataset used</strong>: Collection of Pokémon images organized by species in the final_pokemon_dataset folder</li>
<li><strong>Independent variables</strong>: Image pixel values (RGB channels)</li>
<li><strong>Target variable</strong>: Pokémon species name</li>
<li><strong>Types of models used</strong>: CFAR, CNN, and pre-trained EfficientNetB0</li>
</ul>
</section>
<section id="model-descriptions" class="level2">
<h2 class="anchored" data-anchor-id="model-descriptions">Model Descriptions</h2>
<p>This section provides an overview of the three deep learning models implemented for Pokémon image classification.</p>
<section id="cfar-model" class="level3">
<h3 class="anchored" data-anchor-id="cfar-model">CFAR Model</h3>
<p>The CFAR (Convolutional Filter and Response) model is a lightweight convolutional neural network designed for basic Pokémon image classification.</p>
<ul>
<li><strong>Implementation details</strong>:
<ul>
<li>Library: TensorFlow/Keras</li>
<li>Input shape: 64x64x3 (RGB images)</li>
<li>Architecture:
<ul>
<li>3 convolutional layers (32, 64, and 128 filters)</li>
<li>3 max pooling layers</li>
<li>1 dense hidden layer (256 neurons)</li>
<li>Output layer with softmax activation</li>
</ul></li>
<li>Optimizer: Adam</li>
<li>Loss function: Categorical cross-entropy</li>
<li>Training: 30 epochs with validation after each epoch</li>
</ul></li>
<li><strong>Modeling pipeline</strong>:</li>
</ul>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    subgraph input [Input]
    I[64x64x3 RGB Image]
    end
    
    subgraph conv1 [Layer 1]
    C1[Conv2D 32 Filters] --&gt; MP1[MaxPooling]
    end
    
    subgraph conv2 [Layer 2]
    C2[Conv2D 64 Filters] --&gt; MP2[MaxPooling]
    end
    
    subgraph conv3 [Layer 3]
    C3[Conv2D 128 Filters] --&gt; MP3[MaxPooling]
    end
    
    subgraph dense [Dense Layers]
    F[Flatten] --&gt; D1[Dense 256 neurons]
    D1 --&gt; Out[Output Layer]
    end
    
    I --&gt; C1
    MP1 --&gt; C2
    MP2 --&gt; C3
    MP3 --&gt; F
    
    style input fill:#e1f5fe
    style conv1 fill:#ffecb3
    style conv2 fill:#ffecb3
    style conv3 fill:#ffecb3
    style dense fill:#e8f5e9
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p><em>The diagram above illustrates the architecture of the CFAR model. Starting with an input RGB image (64x64x3), the data flows through three convolutional layers with increasing filter sizes (32, 64, and 128), each followed by a MaxPooling operation to reduce dimensionality. The feature maps from the final convolutional layer are flattened and passed through a dense layer with 256 neurons, which connects to the output layer for classification. This sequential structure enables the model to progressively extract and refine features from the input images before making the final classification decision.</em></p>
<ul>
<li><strong>Code link</strong>: <code>deployment/image_classification/cfar/train_pokemon_classifier_cfar.py</code></li>
<li><strong>Hyperparameters</strong>:
<ul>
<li>Learning rate: Default for Adam optimizer</li>
<li>Batch size: 32</li>
<li>Image size: 64x64 pixels</li>
<li>No data augmentation</li>
</ul></li>
</ul>
</section>
<section id="cnn-model" class="level3">
<h3 class="anchored" data-anchor-id="cnn-model">CNN Model</h3>
<p>The CNN model is a more advanced deep learning architecture with multiple convolutional layers designed for improved feature extraction and classification accuracy.</p>
<ul>
<li><strong>Implementation details</strong>:
<ul>
<li>Library: TensorFlow/Keras</li>
<li>Input shape: 64x64x3 (RGB images)</li>
<li>Architecture:
<ul>
<li>4 convolutional layers with batch normalization and dropout</li>
<li>Each layer has double convolution layers with increasing filter sizes (32→64→128→256)</li>
<li>Max pooling after each layer</li>
<li>Flatten layer followed by dense layer (512 neurons)</li>
<li>Output layer with softmax activation</li>
</ul></li>
<li>Optimizer: Adam with configurable learning rate</li>
<li>Loss function: Categorical cross-entropy</li>
<li>Training: Up to 80 epochs with early stopping and learning rate reduction</li>
</ul></li>
<li><strong>Modeling pipeline</strong>:</li>
</ul>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    subgraph input [Input]
    I[64x64x3 RGB Image]
    end
    
    subgraph aug [Data Augmentation]
    A1[Random Rotation] --&gt; A2[Width/Height Shifts]
    A2 --&gt; A3[Shear Transform]
    A3 --&gt; A4[Zoom]
    A4 --&gt; A5[Horizontal Flip]
    end
    
    subgraph conv1 [Layer 1]
    C1A[Conv2D 32] --&gt; BN1A[BatchNorm]
    BN1A --&gt; C1B[Conv2D 32] 
    C1B --&gt; BN1B[BatchNorm]
    BN1B --&gt; MP1[MaxPooling]
    MP1 --&gt; D1[Dropout 0.25]
    end
    
    subgraph conv2 [Layer 2]
    C2A[Conv2D 64] --&gt; BN2A[BatchNorm]
    BN2A --&gt; C2B[Conv2D 64] 
    C2B --&gt; BN2B[BatchNorm]
    BN2B --&gt; MP2[MaxPooling]
    MP2 --&gt; D2[Dropout 0.25]
    end
    
    subgraph conv3 [Layer 3]
    C3A[Conv2D 128] --&gt; BN3A[BatchNorm]
    BN3A --&gt; C3B[Conv2D 128] 
    C3B --&gt; BN3B[BatchNorm]
    BN3B --&gt; MP3[MaxPooling]
    MP3 --&gt; D3[Dropout 0.25]
    end
    
    subgraph conv4 [Layer 4]
    C4A[Conv2D 256] --&gt; BN4A[BatchNorm]
    BN4A --&gt; C4B[Conv2D 256] 
    C4B --&gt; BN4B[BatchNorm]
    BN4B --&gt; MP4[MaxPooling]
    MP4 --&gt; D4[Dropout 0.25]
    end
    
    subgraph dense [Dense Layers]
    F[Flatten] --&gt; DL1[Dense 512]
    DL1 --&gt; BN5[BatchNorm]
    BN5 --&gt; D5[Dropout 0.5]
    D5 --&gt; Out[Output Layer]
    end
    
    I --&gt; A1
    A5 --&gt; C1A
    D1 --&gt; C2A
    D2 --&gt; C3A
    D3 --&gt; C4A
    D4 --&gt; F
    
    style input fill:#e1f5fe
    style aug fill:#e8f5e9
    style conv1 fill:#ffecb3
    style conv2 fill:#ffe082
    style conv3 fill:#ffd54f
    style conv4 fill:#ffca28
    style dense fill:#e8f5e9
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p><em>The diagram above illustrates the more complex CNN architecture used for Pokémon classification. This model features four convolutional layers with progressively increasing filter sizes (32→64→128→256), each implementing a double convolution pattern. The pipeline begins with data augmentation techniques (rotation, shifts, shear, zoom, and flipping) that enhance the training dataset and improve generalization. Every layer follows the same structure: two convolutional layers, each followed by batch normalization, then max pooling, and finally dropout (0.25) to prevent overfitting. After the convolutional layers, the data is flattened and passed through a dense layer with 512 neurons, followed by batch normalization and a higher dropout rate (0.5) before the final classification layer. This architecture significantly outperforms the simpler CFAR model by incorporating modern deep learning techniques like batch normalization and employing a deeper structure for more sophisticated feature extraction.</em></p>
<ul>
<li><strong>Code link</strong>: <code>deployment/image_classification/CNN/train_pokemon_classifier_cnn.py</code></li>
<li><strong>Hyperparameters</strong>:
<ul>
<li>Learning rate: 0.001 (with reduction on plateau)</li>
<li>Batch size: 32</li>
<li>Image size: 64x64 pixels</li>
<li>Dropout rates: 0.25 for convolutional layers, 0.5 for dense layer</li>
<li>Data augmentation parameters: rotation, shifts, shear, zoom, horizontal flip</li>
</ul></li>
</ul>
</section>
<section id="pre-trained-cnn-model-efficientnetb0-transfer-learning" class="level3">
<h3 class="anchored" data-anchor-id="pre-trained-cnn-model-efficientnetb0-transfer-learning">Pre-trained CNN Model (EfficientNetB0, Transfer Learning)</h3>
<p>The third and most effective model uses <strong>EfficientNetB0</strong>, a modern and scalable convolutional neural network architecture pre-trained on the ImageNet dataset. Through <strong>transfer learning</strong>, the model leverages learned visual features and adapts them to the Pokémon image classification task with minimal training from scratch.</p>
<p>Training was conducted in two phases:</p>
<ul>
<li><strong>Phase 1</strong>: The EfficientNetB0 base was frozen, and only the custom classification head was trained.<br>
</li>
<li><strong>Phase 2</strong>: The base model was partially unfrozen and fine-tuned using a lower learning rate.</li>
</ul>
<p>To improve generalization, a strong <strong>data augmentation pipeline</strong> was applied. This increased the model’s robustness to real-world image variability such as different lighting, angles, and scales.</p>
<ul>
<li><strong>Augmentation techniques included</strong>:
<ul>
<li>Random horizontal flipping<br>
</li>
<li>Random rotation<br>
</li>
<li>Random zoom<br>
</li>
<li>Contrast adjustment<br>
</li>
<li>Brightness adjustment</li>
</ul></li>
</ul>
<p>The model architecture consists of EfficientNetB0 (with <code>include_top=False</code>) as the feature extractor, followed by:</p>
<ul>
<li>Global Average Pooling to reduce dimensionality<br>
</li>
<li>A Dense layer with 128 neurons and ReLU activation<br>
</li>
<li>Dropout layer (rate: 0.5) to prevent overfitting<br>
</li>
<li>A Softmax output layer for multi-class classification</li>
</ul>
<section id="technical-summary" class="level4">
<h4 class="anchored" data-anchor-id="technical-summary">Technical Summary</h4>
<ul>
<li><strong>Input shape</strong>: 224x224x3 RGB images<br>
</li>
<li><strong>Loss function</strong>: Sparse categorical cross-entropy<br>
</li>
<li><strong>Optimizer</strong>: Adam
<ul>
<li>Learning rate: 1e-4 (frozen base), 1e-5 (fine-tuning)<br>
</li>
</ul></li>
<li><strong>Batch size</strong>: 16<br>
</li>
<li><strong>Training epochs</strong>: 40 (early stopping applied)<br>
</li>
<li><strong>Callbacks</strong>:
<ul>
<li>EarlyStopping with <code>patience=5</code> and weight restoration</li>
<li>Custom callback <code>SaveEveryN</code> saving checkpoints every 10 epochs</li>
</ul></li>
</ul>
</section>
<section id="model-pipeline" class="level4">
<h4 class="anchored" data-anchor-id="model-pipeline">Model Pipeline</h4>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    subgraph input [Input]
    I[224x224x3 RGB Image]
    end
 
    subgraph aug [Data Augmentation]
    A1[Random Flip] --&gt; A2[Random Rotation]
    A2 --&gt; A3[Random Zoom]
    A3 --&gt; A4[Random Contrast]
    A4 --&gt; A5[Random Brightness]
    end
 
    subgraph preproc [Preprocessing]
    P[EfficientNet Preprocessing]
    end
 
    subgraph base [Base Model]
    EB[EfficientNetB0 Pretrained\non ImageNet]
    end
 
    subgraph head [Classification Head]
    GAP[Global Average Pooling] --&gt; D1[Dense 128]
    D1 --&gt; DO[Dropout 0.5]
    DO --&gt; Out[Output Layer]
    end
 
    subgraph training [Training Process]
    T1[Phase 1: Frozen Base\n40 Epochs, LR=1e-4] --&gt; T2[Phase 2: Fine-tuning\nLower LR=1e-5]
    end
 
    I --&gt; A1
    A5 --&gt; P
    P --&gt; EB
    EB --&gt; GAP
    Out --&gt; training
 
    style input fill:#e1f5fe
    style aug fill:#fff9c4
    style preproc fill:#f3e5f5
    style base fill:#ffccbc
    style head fill:#e8f5e9
    style training fill:#e0f2f1
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p><em>This diagram shows the entire processing pipeline: the input image is augmented, preprocessed, passed through the EfficientNetB0 base model, and finally classified through a custom dense head. The two-phase training approach ensures both rapid convergence and effective fine-tuning.</em></p>
</section>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>The performance of each model varies based on their architecture complexity and training approach. Our model development followed an iterative process: we first implemented the lightweight CFAR model alongside a basic CNN architecture. Based on the promising results of the CNN approach, we further refined the CNN architecture by adding batch normalization, dropout layers, and implementing double convolution patterns. As a third strategy, we adopted transfer learning by training an EfficientNetB0 pre-trained model to expand our comparison of different deep learning approaches.</p>
<section id="cfar-model-1" class="level3">
<h3 class="anchored" data-anchor-id="cfar-model-1">CFAR Model</h3>
<ul>
<li>Training accuracy: ~95%</li>
<li>Validation accuracy: ~24%</li>
<li>Simple architecture leads to faster training but lower accuracy</li>
<li>Best suited for quick prototyping or when computational resources are limited</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="pics/training_history_cfar.png" class="img-fluid figure-img"></p>
<figcaption>Training History for CFAR Model</figcaption>
</figure>
</div>
<p><em>The figure above shows the training history of the CFAR model over 30 epochs. The left plot displays accuracy metrics, where training accuracy (blue line) steadily increases to approximately 95% while validation accuracy (orange line) plateaus around 24%, indicating significant overfitting. The right plot shows loss metrics, with training loss (blue) decreasing to near zero while validation loss (orange) increases dramatically after the initial epochs, further confirming the model’s poor generalization abilities.</em></p>
</section>
<section id="cnn-model-1" class="level3">
<h3 class="anchored" data-anchor-id="cnn-model-1">CNN Model</h3>
<ul>
<li>Training accuracy: ~70-75%</li>
<li>Validation accuracy: ~65%</li>
<li>The addition of batch normalization and dropout layers significantly reduces overfitting</li>
<li>Multiple convolutional layers allow for hierarchical feature learning</li>
<li>Early stopping and learning rate reduction help optimize training</li>
<li>Visualization tools provide insights into training progress</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="pics/training_history_cnn.png" class="img-fluid figure-img"></p>
<figcaption>Training History for CNN Model</figcaption>
</figure>
</div>
<p><em>The figure above illustrates the CNN model’s training history over 50 epochs. The accuracy plot (left) shows a more balanced progression between training (blue) and test/validation (orange) accuracy, both reaching approximately 70-75%. The loss plot (right) demonstrates how both training and test losses decrease steadily and remain relatively close together throughout training. This indicates the CNN model achieves better generalization performance than the CFAR model, with the regularization techniques (batch normalization and dropout) effectively mitigating overfitting.</em></p>
</section>
<section id="pre-trained-cnn-model" class="level3">
<h3 class="anchored" data-anchor-id="pre-trained-cnn-model">Pre-trained CNN Model</h3>
<ul>
<li>Training accuracy: ~45%</li>
<li>Validation accuracy: ~65%</li>
<li>Two-phase training approach allows for fine-tuning while avoiding catastrophic forgetting</li>
<li>Higher resolution input (224x224) captures more details but requires more computational resources</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="pics/training_history_EfficientNet.png" class="img-fluid figure-img"></p>
<figcaption>Accuracy over Epochs for Pre-trained EfficientNetB0 Model</figcaption>
</figure>
</div>
<p><em>This graph shows the EfficientNetB0 model’s training performance over 50 epochs. The validation accuracy (orange) improves rapidly in early epochs, demonstrating transfer learning’s efficiency. We observe a drop at epoch 40 during the transition from Phase 1 (frozen base) to Phase 2 (fine-tuning), followed by quick recovery. Final validation accuracy reaches ~65%, confirming our two-phase approach effectively adapts pre-trained knowledge to Pokémon classification while preventing catastrophic forgetting.</em></p>
</section>
</section>
<section id="model-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="model-interpretation">Model Interpretation</h2>
<p>Our experiments with three architectures yield clear insights: - EfficientNetB0 currently performs similarly to our CNN (~65% validation accuracy), but has significantly greater potential for improvement through further fine-tuning - CNN offers balanced performance-resource trade-off (65% accuracy with moderate training time) - CFAR provides rapid prototyping capability but suffers from significant overfitting - Data augmentation is essential for all models to generalize effectively - EfficientNetB0’s pre-trained knowledge can be better leveraged with more extensive adaptation techniques</p>
</section>
<section id="conclusions-and-next-steps" class="level2">
<h2 class="anchored" data-anchor-id="conclusions-and-next-steps">Conclusions and Next Steps</h2>
<p>While both CNN and EfficientNetB0 models currently achieve similar performance (~65% validation accuracy), we recommend the EfficientNetB0 model for deployment due to its greater potential for optimization. Our three-model approach demonstrates that transfer learning offers a promising path forward for Pokémon image classification, especially with additional fine-tuning.</p>
<section id="limitations" class="level3">
<h3 class="anchored" data-anchor-id="limitations">Limitations</h3>
<ul>
<li>Current performance of all models depends on dataset quality and diversity</li>
<li>Visually similar Pokémon remain challenging to differentiate</li>
<li>EfficientNetB0 requires more computational resources but has significant room for improvement</li>
<li>Further fine-tuning of the pre-trained layers is necessary to fully leverage ImageNet knowledge</li>
</ul>
</section>
<section id="future-work" class="level3">
<h3 class="anchored" data-anchor-id="future-work">Future Work</h3>
<ul>
<li>More extensive fine-tuning of EfficientNetB0, especially unfreezing more layers gradually</li>
<li>Expand dataset diversity with more poses and environments</li>
<li>Explore more advanced transfer learning techniques to better adapt ImageNet knowledge</li>
<li>Optimize for mobile via model quantization</li>
<li>Implement more sophisticated data augmentation techniques specifically for Pokémon images</li>
<li>Develop model ensembles combining CNN and EfficientNetB0 strengths</li>
</ul>
</section>
<section id="deployment" class="level3">
<h3 class="anchored" data-anchor-id="deployment">Deployment</h3>
<ul>
<li>GUI testing application already developed (<code>pokemon_classifier_app.py</code>) for internal validation</li>
<li>Web-based interface for online Pokémon recognition via our website</li>
</ul>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/daai25\.github\.io\/PokemonGOBattleAssistant\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>